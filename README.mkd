Required tools and libraries
-----------------------------

Download and install nix from <https://nixos.org/download.html> (install nix, not nixOs)

Get the `trec-car-create` code from <https://github.com/TREMA-UNH/trec-car-create/>, follow the [instructions](https://github.com/TREMA-UNH/trec-car-create/blob/main/README.mkd) for compilation, and return back here.


Create new release directory
----------------------------

Check out this repository and create a directory to collect your release in,
```bash
lang=en
ver=1.6
git clone git@github.com:TREMA-UNH/trec-car-release.git release-v$lang$ver
cd release-v$lang$ver
```

Set up
---------

Set `$bin` to the location of whree the `trec-car-create` binaries reside. For instance,
```bash
export bin=$(pwd)/car-tools/bin
```

Set the variable `$toolsrepo` to the location of the trec-car-tools working directory

Make the `trec-car-create` tools available to the data-set build pipeline by calling:

```bash
./update-tools.sh ./$configFile
```

Where `$configFile` is the file `config.$lang.nix` you will be configuring in the next step. To set up, you can use `config.en.nix`. (It is important to include  `./`)

Background: semantic changes in our `trec-car-create` tools are reflected in the version report by the `--tool-version` flag supported by each tool. Should the version change, necessary parts of the pipeline will be automatically re-run.


Configure
---------

Edit (or create) your language specific configuration file `config.*.nix` to adjust attributes

Here an example for english (`en`):

* `globalConfig.dump_date = "20220101"`  // yyyymmdd
* `globalConfig.version = "v2.6"`

You separately need `config.*.yaml` which defines language-specific template resolution of Wikipedia. If you are using a language that does not have an existing `config.$lang.yaml`, please create a matching language configuration based on our examples. Please use the same language mnemonics as Wikipedia.

In subsequent calls to `.run.sh` you will be passing in this config file on the command line. Alternatively you can hard code your configuration choice by changing the top of the file `default.nix` as follows:

```nix
configFile ? ./config.en.nix   # change to a different file name
```


Fetching Dumpstatus
---------------------

Finally, fetch the `dumpstatus.json` file of the dump which you are going to
ingest (e.g. <https://dumps.wikimedia.org/enwiki/20220101/dumpstatus.json>) and place it in
the `trec-car-release/` directory (named `dumpstatus.json`).

Our pipeline will download required files from the Wikipedia mirror.

### Deduplication

Parargraph deduplication is enabled by default. You can switch off deduplication of similar paragraphs by setting `deduplicate ? false`. 

We are using GloVE as part of our deduplication pipeline. If you are using with a different language, you will need to download and configure your language-specific GloVE version in `default.nix` below comment `# GloVe embeddings` 

### Wikidata

Identify the wikidata dump that is closest to the Wikipedia dump, we call it `${wikidumpdate}` below

Download the wikidata dump with the following command and take note of the SHA

`nix-prefetch-url https://dumps.wikimedia.your.org/wikidatawiki/entities/${wikidumpdate}/wikidata-${wikidumpdate}-all.json.bz2

Configure the matching wikidata dump date and file SHA in `config.*.nix`, example

* wikidata_dump_date = "20220103";
* wikidata_dump_sha256 = "1dzxm740gm74wnb96bb8829gygkmqwwpzihbrljzrddr74hfpnch";


Customize your conversion
--------------------------

The conversion will commence in the following passes. 

1. dumps are downloaded and converted into raw pages
2. content pages are identified, using predicates listed in `${globalConfig.dropPagesWithPrefix}` (all else)
3. information from redirects and disambiguation pages are distilled into the metadata of article pages
4. article pages are filtered into filteredPages. All pages that match the following predicates  are removed `${config.filterPagesWithPrefix}` and `${config.filterCategories}`. Furthermore all disambiguation pages and redirect pages are removed. The predicate list can be extended with `${config.filterpredicates}` (note only pages matching these predicates will be considered, unless inverted with the not operator) 
5. benchmarks are derived from the filtered pages.

The exact syntax for predicates can be taken from `./car-tools/trec-car-filter -h`



Run conversion
--------------

While technically you can call `nix-build` directly, we recommend you call it through the provided `run.sh` script.

The following commands support a test mode (where you will only use the first file of the Wikipedia dump) by adding the parameter `--arg dumpTest true`.

You will have to pass in the config file with `--arg configFile ./config.en.nix`

To limit the number of cores used by the conversion to `$n` by adding the parameter `--cores $n` 

To enable deduplication run with `--arg deduplicate True`.


### Create Wikipedia dump

To produce a car-style dump of Wikipedia. Necessary files will automatically be downloaded. 

```bash
./run.sh dump --arg configFile $configFile
```

your results will be collected in `result-dump`


### Create full data set

Create a dump with filtering, deduplication, outlines, paragraph corpus, and automatic ground truth, call

`./run.sh all --arg configFile $configFile`

your links to your results will be collected in  `result-all`


We recommend you first build a dump, then rerun with `all`. The dump will be automatically reused.



Save built data and clean up
-----------------------------


When data is built, it still resides inside the nix store. We recommend to archive it once done and clean the nix store.

copy data out of result directory using "-L" to dereference softlinks
```
rsync -L result-all $yourTargetDir
```

nix tracks which files can be purged from its store through softlinks. Only when the last link (created through nix) is deleted, it will remove the build products. Until then it slowly eats up all hard drive space.

Delete target directories of this (and previous failed attempts) in results/...-$date

call the garbage collector of nix

```
nix-gc
```


UNH only: Upload data to trec-car.cs.unh.edu
---------------------------------------------
rsync -a *v?.?*zip lava:trec-car/public_html/datareleases



